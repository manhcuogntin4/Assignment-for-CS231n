{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network. The net has an input dimension of\n",
    "  N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "  We train the network with a softmax loss function and L2 regularization on the\n",
    "  weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "  connected layer.\n",
    "  In other words, the network has the following architecture:\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights are initialized to small random values and\n",
    "    biases are initialized to zero. Weights and biases are stored in the\n",
    "    variable self.params, which is a dictionary with the following keys:\n",
    "    W1: First layer weights; has shape (D, H)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (H, C)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "    Inputs:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a two layer fully connected neural\n",
    "    network.\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "      is not passed then we only return scores, and if it is passed then we\n",
    "      instead return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "    Returns:\n",
    "    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "    If y is not None, instead return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "    #############################################################################\n",
    "    # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "    # Store the result in the scores variable, which should be an array of      #\n",
    "    # shape (N, C).                                                             #\n",
    "    #############################################################################\n",
    "    h_output = np.maximum(0, X.dot(W1) + b1) #(N,D) * (D,H) = (N,H)\n",
    "    print(\"h_output\",h_output.shape)\n",
    "    scores = h_output.dot(W2) + b2\n",
    "    print(\"scores\", score.shape)\n",
    "    # pass\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "    \n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = None\n",
    "    #############################################################################\n",
    "    # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "    # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "    # classifier loss. So that your results match ours, multiply the            #\n",
    "    # regularization loss by 0.5                                                #\n",
    "    #############################################################################\n",
    "    shift_scores = scores - np.max(scores, axis = 1).reshape(-1,1)\n",
    "    print \"shift_scores:\",shift_scores.shape, shift_scores\n",
    "    softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores), axis = 1).reshape(-1,1)\n",
    "    loss = -np.sum(np.log(softmax_output[range(N), list(y)]))\n",
    "    loss /= N\n",
    "    loss +=  0.5* reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "    print(\"loss\",loss)\n",
    "    # pass\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    grads = {}\n",
    "    #############################################################################\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "    #############################################################################\n",
    "    # pass\n",
    "    dscores = softmax_output.copy()\n",
    "    dscores[range(N), list(y)] -= 1\n",
    "    dscores /= N\n",
    "    grads['W2'] = h_output.T.dot(dscores) + reg * W2\n",
    "    grads['b2'] = np.sum(dscores, axis = 0)\n",
    "    \n",
    "    dh = dscores.dot(W2.T)\n",
    "    dh_ReLu = (h_output > 0) * dh\n",
    "    grads['W1'] = X.T.dot(dh_ReLu) + reg * W1\n",
    "    grads['b1'] = np.sum(dh_ReLu, axis = 0)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in xrange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "      # them in X_batch and y_batch respectively.                             #\n",
    "      #########################################################################\n",
    "      idx = np.random.choice(num_train, batch_size, replace=True)\n",
    "      X_batch = X[idx]\n",
    "      y_batch = y[idx]\n",
    "      # pass\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "      # parameters of the network (stored in the dictionary self.params)      #\n",
    "      # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "      # stored in the grads dictionary defined above.                         #\n",
    "      #########################################################################\n",
    "      self.params['W2'] += - learning_rate * grads['W2']\n",
    "      self.params['b2'] += - learning_rate * grads['b2']\n",
    "      self.params['W1'] += - learning_rate * grads['W1']\n",
    "      self.params['b1'] += - learning_rate * grads['b1']\n",
    "      # pass\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "    y_pred = None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement this function; it should be VERY simple!                #\n",
    "    ###########################################################################\n",
    "    h = np.maximum(0, X.dot(self.params['W1']) + self.params['b1'])\n",
    "    scores = h.dot(self.params['W2']) + self.params['b2']\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    # pass\n",
    "    ###########################################################################\n",
    "    #                              END OF YOUR CODE                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl=TwoLayerNet(10,8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['b2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.random.randn(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=np.random.randint(3, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 0, 2, 1, 1,\n",
       "       1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 1,\n",
       "       2, 2, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 2, 2, 1, 1, 1, 2,\n",
       "       1, 1, 0, 2, 0, 0, 1, 0, 2, 1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 2, 0, 2,\n",
       "       1, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h_output', (100, 8), array([[0.00000000e+00, 8.25851792e-05, 3.36501434e-04, 0.00000000e+00,\n",
      "        3.22907148e-04, 2.35159505e-04, 1.85172188e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 6.20512517e-06, 0.00000000e+00, 5.28125045e-04,\n",
      "        1.20260977e-04, 6.32025091e-04, 3.07647863e-04, 0.00000000e+00],\n",
      "       [7.99564696e-04, 0.00000000e+00, 0.00000000e+00, 3.97930895e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 7.50450185e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.15595681e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 4.19026099e-04, 3.00388697e-04],\n",
      "       [7.62633547e-04, 3.40409224e-04, 0.00000000e+00, 1.22560272e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [2.03318073e-04, 5.13571626e-04, 3.51889586e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.25365906e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.95195877e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.13283041e-04, 3.88227171e-04],\n",
      "       [3.73737887e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 4.15087291e-04, 1.13318399e-04],\n",
      "       [0.00000000e+00, 2.36012280e-04, 4.11475537e-04, 6.36264072e-04,\n",
      "        1.02523807e-03, 6.81067683e-04, 0.00000000e+00, 2.12769930e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.55246648e-04, 1.51985044e-04,\n",
      "        0.00000000e+00, 1.46753537e-04, 3.64244372e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.11733223e-05,\n",
      "        0.00000000e+00, 5.37656275e-04, 1.36765031e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.10774970e-04,\n",
      "        0.00000000e+00, 4.28474392e-04, 2.50649776e-05, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        3.61463779e-04, 0.00000000e+00, 0.00000000e+00, 2.07325798e-04],\n",
      "       [4.18214390e-04, 2.97098525e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.44126643e-05],\n",
      "       [1.49702264e-04, 6.68299697e-05, 2.12342959e-04, 4.70181145e-04,\n",
      "        0.00000000e+00, 2.31149146e-04, 7.01106652e-05, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.02466492e-04,\n",
      "        3.13148676e-04, 3.47603627e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 2.48440191e-04, 2.16644518e-04, 0.00000000e+00,\n",
      "        1.38716340e-04, 2.34090139e-04, 0.00000000e+00, 2.71490071e-04],\n",
      "       [0.00000000e+00, 1.75721947e-04, 3.03567080e-05, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.16316706e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.24080456e-05,\n",
      "        8.29323613e-05, 3.22574097e-04, 6.76838046e-04, 1.31049318e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.11955975e-04,\n",
      "        1.77901929e-04, 0.00000000e+00, 2.80349706e-04, 0.00000000e+00],\n",
      "       [2.93704405e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 2.66269675e-04, 6.93104214e-04],\n",
      "       [0.00000000e+00, 1.59531002e-04, 2.03614957e-06, 1.97981539e-04,\n",
      "        3.81820870e-04, 8.15463900e-04, 4.71821921e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.86979544e-04,\n",
      "        8.17495674e-05, 2.10211500e-04, 0.00000000e+00, 1.27695634e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 4.05814313e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.57887626e-04],\n",
      "       [2.87745216e-04, 3.32576107e-04, 4.49721336e-04, 1.45933672e-04,\n",
      "        1.88062267e-04, 3.82115531e-05, 5.04178541e-04, 8.84545269e-05],\n",
      "       [1.86682980e-04, 0.00000000e+00, 4.11247338e-05, 2.20731715e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.63673422e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.57137263e-04,\n",
      "        0.00000000e+00, 1.48335389e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 2.67711269e-05, 6.18045094e-04,\n",
      "        0.00000000e+00, 4.86534855e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 1.33672136e-04, 4.42845597e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.98015815e-04, 0.00000000e+00, 9.87448511e-05],\n",
      "       [3.53740510e-04, 1.13033099e-04, 0.00000000e+00, 4.48554587e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 4.92695828e-05, 5.98585020e-05, 0.00000000e+00,\n",
      "        6.59807082e-05, 0.00000000e+00, 0.00000000e+00, 4.80535249e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00849457e-05,\n",
      "        8.54844433e-05, 2.71340034e-04, 4.59914958e-04, 1.32460131e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.22209464e-04, 0.00000000e+00, 4.21265950e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.21333180e-04, 2.97043141e-04,\n",
      "        2.26098281e-05, 0.00000000e+00, 0.00000000e+00, 4.07582002e-04],\n",
      "       [3.31089058e-04, 0.00000000e+00, 0.00000000e+00, 2.44646751e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 5.63529239e-05, 4.10526469e-04,\n",
      "        3.90728670e-05, 4.88670906e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [5.09691743e-05, 2.82109162e-04, 1.63370631e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 2.66805038e-04, 6.38957589e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 2.87407414e-04, 4.78551115e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.85747899e-04, 0.00000000e+00, 2.64631343e-04],\n",
      "       [1.83973593e-04, 1.37781344e-04, 3.44164007e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.62939039e-04],\n",
      "       [0.00000000e+00, 1.73799431e-04, 3.62335766e-04, 0.00000000e+00,\n",
      "        1.68867655e-04, 6.40082933e-04, 0.00000000e+00, 2.02206767e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.80408382e-04, 0.00000000e+00,\n",
      "        4.46242720e-04, 1.92225968e-04, 0.00000000e+00, 5.31924397e-04],\n",
      "       [8.60977173e-05, 9.68685617e-05, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 7.81146905e-05, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.97091892e-05, 0.00000000e+00,\n",
      "        0.00000000e+00, 3.75500538e-04, 0.00000000e+00, 3.85473667e-04],\n",
      "       [2.86056393e-04, 0.00000000e+00, 1.99821165e-04, 1.38636680e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 5.06924929e-04, 1.99985231e-04],\n",
      "       [0.00000000e+00, 4.26229321e-04, 1.20140894e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.47426862e-04, 6.62083882e-04, 2.97612899e-04],\n",
      "       [0.00000000e+00, 2.35857030e-04, 1.23113159e-04, 0.00000000e+00,\n",
      "        5.08261380e-06, 8.49809793e-04, 3.97363938e-04, 0.00000000e+00],\n",
      "       [2.49726596e-04, 3.15023440e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.54216439e-04],\n",
      "       [6.53220480e-04, 0.00000000e+00, 0.00000000e+00, 4.22699210e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [4.73960795e-04, 2.68230221e-04, 0.00000000e+00, 2.47299255e-04,\n",
      "        2.88971165e-04, 1.26498216e-04, 4.76999913e-05, 0.00000000e+00],\n",
      "       [2.09395315e-04, 5.75238053e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 4.46684579e-04, 3.36725450e-04, 1.36488348e-04],\n",
      "       [6.21255702e-05, 1.88754771e-04, 7.25730327e-05, 1.12678857e-04,\n",
      "        3.07844680e-04, 1.77166858e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 1.05523663e-04, 2.71317239e-04, 0.00000000e+00,\n",
      "        1.74076520e-04, 1.41541260e-04, 0.00000000e+00, 5.59194723e-04],\n",
      "       [0.00000000e+00, 1.59808321e-04, 9.61257081e-05, 0.00000000e+00,\n",
      "        4.21332403e-04, 1.42179860e-04, 0.00000000e+00, 5.24983538e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.18393490e-04, 1.58357147e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [7.15060175e-05, 0.00000000e+00, 1.39157854e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.38272070e-04, 5.71403295e-05],\n",
      "       [1.68680227e-06, 2.29545987e-04, 0.00000000e+00, 4.63936171e-05,\n",
      "        0.00000000e+00, 4.46239772e-05, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.47687193e-04,\n",
      "        0.00000000e+00, 8.31864738e-05, 3.72556307e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 2.58187467e-04, 3.26294206e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 3.40034279e-04, 9.94956805e-05],\n",
      "       [3.79764687e-04, 1.68981448e-04, 0.00000000e+00, 3.60001086e-04,\n",
      "        1.79452604e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "       [1.02692584e-04, 0.00000000e+00, 0.00000000e+00, 3.28634442e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 1.85540121e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.10978033e-04,\n",
      "        1.44112175e-04, 5.37838169e-04, 7.59280482e-05, 0.00000000e+00],\n",
      "       [2.92102252e-04, 0.00000000e+00, 6.62524454e-05, 4.15189498e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.23724136e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.91766237e-04,\n",
      "        0.00000000e+00, 1.22685978e-04, 3.55419865e-04, 0.00000000e+00],\n",
      "       [2.37984600e-04, 1.21236513e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 9.99311878e-05, 6.20274649e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.28624553e-04,\n",
      "        2.20370116e-05, 3.07154118e-04, 1.58665948e-04, 5.28190056e-04],\n",
      "       [3.11790119e-04, 2.68858686e-07, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.39323447e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.13931533e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 4.69389033e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 2.41229596e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.03544176e-04],\n",
      "       [5.55989085e-04, 4.81835077e-04, 8.00764366e-06, 0.00000000e+00,\n",
      "        1.83460662e-06, 0.00000000e+00, 5.45023811e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 4.71854096e-05, 3.40996593e-04,\n",
      "        0.00000000e+00, 2.78852677e-04, 0.00000000e+00, 2.89675315e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 2.30191377e-04, 0.00000000e+00,\n",
      "        1.07224446e-04, 5.88416030e-04, 0.00000000e+00, 2.40321333e-04],\n",
      "       [2.02698722e-05, 0.00000000e+00, 0.00000000e+00, 1.38802163e-04,\n",
      "        3.21523162e-05, 1.25239673e-05, 6.92622030e-05, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 2.32583704e-05, 6.48766347e-04,\n",
      "        1.24993370e-04, 4.19571957e-04, 3.51596194e-04, 1.27549716e-04],\n",
      "       [0.00000000e+00, 4.94401923e-05, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.11842210e-04, 2.02500401e-04, 6.02897966e-04, 0.00000000e+00],\n",
      "       [1.33316675e-04, 0.00000000e+00, 2.79650269e-04, 2.24310139e-04,\n",
      "        0.00000000e+00, 0.00000000e+00, 6.15792943e-05, 9.94507869e-05],\n",
      "       [6.95821275e-04, 2.81349795e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 6.21189127e-04, 2.12719947e-04],\n",
      "       [1.44086038e-05, 6.46889915e-05, 0.00000000e+00, 6.86678464e-04,\n",
      "        2.93402827e-04, 7.00578904e-04, 4.04601766e-04, 0.00000000e+00],\n",
      "       [2.69331500e-04, 5.83241147e-04, 1.98144392e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 7.26986393e-07, 2.13218460e-04, 0.00000000e+00],\n",
      "       [1.41981755e-04, 1.13689704e-04, 3.67129829e-05, 1.93157585e-04,\n",
      "        2.18355930e-04, 1.99023639e-05, 1.41529500e-04, 3.37714435e-04],\n",
      "       [4.92450741e-04, 0.00000000e+00, 0.00000000e+00, 5.15432266e-04,\n",
      "        5.31527932e-04, 0.00000000e+00, 2.54057146e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.20681851e-05, 2.53346247e-04,\n",
      "        2.28458122e-04, 3.06164704e-05, 0.00000000e+00, 1.53512076e-04],\n",
      "       [1.97905699e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.21477265e-04],\n",
      "       [2.71644886e-04, 9.08849715e-05, 0.00000000e+00, 2.81901308e-04,\n",
      "        5.30363734e-04, 7.27878075e-05, 1.41591236e-04, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.31247054e-04,\n",
      "        3.02215059e-04, 5.73953018e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.03163417e-04, 2.17565678e-04,\n",
      "        0.00000000e+00, 1.02958116e-04, 0.00000000e+00, 6.76323687e-04],\n",
      "       [0.00000000e+00, 1.45498281e-04, 3.26969788e-04, 0.00000000e+00,\n",
      "        1.89795264e-04, 0.00000000e+00, 0.00000000e+00, 4.45433552e-04],\n",
      "       [2.73160064e-04, 1.62077093e-04, 3.37481732e-05, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.06838600e-04],\n",
      "       [1.63744349e-04, 6.54806274e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 7.93816111e-05, 0.00000000e+00, 1.81460416e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.32723929e-04,\n",
      "        7.29387278e-04, 4.97922659e-04, 5.93733892e-04, 4.67609682e-05],\n",
      "       [1.59916489e-05, 1.10926494e-04, 6.20336996e-05, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.99971579e-05],\n",
      "       [4.30833193e-05, 0.00000000e+00, 0.00000000e+00, 4.83735778e-04,\n",
      "        2.86804484e-04, 6.18118550e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [8.58809755e-04, 2.58420133e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        1.48873505e-05, 0.00000000e+00, 4.31443741e-05, 0.00000000e+00],\n",
      "       [4.38598633e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 5.73929423e-04, 2.49911349e-04],\n",
      "       [0.00000000e+00, 3.89231326e-05, 0.00000000e+00, 2.43612804e-05,\n",
      "        1.83710147e-04, 4.43647322e-04, 0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 8.58323865e-05, 0.00000000e+00,\n",
      "        3.02640777e-04, 0.00000000e+00, 0.00000000e+00, 3.96376968e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.28126022e-04, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.64761966e-04],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 1.79504292e-04, 2.65077581e-04, 3.19156982e-04],\n",
      "       [9.26625087e-04, 2.07868692e-04, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00, 5.53663224e-04, 0.00000000e+00],\n",
      "       [3.79507240e-04, 7.04091934e-04, 4.67216792e-04, 0.00000000e+00,\n",
      "        5.50692061e-04, 0.00000000e+00, 0.00000000e+00, 1.83713106e-04],\n",
      "       [7.98459240e-05, 0.00000000e+00, 1.91915955e-05, 1.01559745e-03,\n",
      "        0.00000000e+00, 7.06489683e-04, 0.00000000e+00, 0.00000000e+00]]))\n",
      "('scores', array([[-1.00766205e-07, -4.69562925e-08,  4.77676767e-08],\n",
      "       [ 3.20896651e-08,  1.78197562e-09, -5.47994153e-08],\n",
      "       [ 5.12196723e-09, -4.52058725e-08, -1.34209203e-07],\n",
      "       [ 1.07479256e-08, -3.06778226e-08, -3.33860417e-08],\n",
      "       [-1.29199698e-07,  5.97141338e-08, -5.33011007e-08],\n",
      "       [-1.66163772e-07, -1.94769982e-08,  7.22435774e-08],\n",
      "       [-1.67608627e-09, -4.82746564e-08, -5.32894860e-09],\n",
      "       [-2.01744565e-08,  2.04359518e-08, -6.39254835e-08],\n",
      "       [-2.27923031e-07, -7.47369666e-08,  4.25713262e-08],\n",
      "       [ 1.25090698e-08, -6.30475118e-08, -2.97799187e-10],\n",
      "       [-5.60814191e-09,  1.00074965e-07, -2.41393943e-08],\n",
      "       [-6.15176403e-09,  7.17871215e-08, -9.63040957e-09],\n",
      "       [-6.69397862e-08,  9.55975148e-09, -1.34933656e-08],\n",
      "       [-1.02129643e-07,  4.88896066e-08, -2.71755109e-08],\n",
      "       [-3.58423436e-08, -6.37441259e-08,  3.81537644e-08],\n",
      "       [-3.23011426e-08,  2.52436829e-08, -2.67705267e-08],\n",
      "       [-1.33732910e-07,  2.44430431e-08,  5.69521520e-08],\n",
      "       [-4.31882203e-08, -3.60046655e-09,  1.00236344e-08],\n",
      "       [ 3.32593824e-08,  3.05004783e-10, -8.26758217e-08],\n",
      "       [ 2.54395876e-08, -7.90530540e-08, -4.51322624e-08],\n",
      "       [-1.04220710e-07,  9.90924007e-08, -1.15755831e-08],\n",
      "       [-2.98052418e-08,  6.14393533e-08, -9.73243623e-08],\n",
      "       [-1.32113468e-08,  5.82752071e-09, -2.57019738e-09],\n",
      "       [-1.28125201e-07, -2.39238624e-08,  1.35701198e-07],\n",
      "       [-1.36131141e-07, -1.33553746e-07,  3.62238181e-08],\n",
      "       [-3.76850491e-08, -8.67923328e-10,  9.28859691e-09],\n",
      "       [ 2.05579491e-08, -3.32917414e-08, -2.95859878e-09],\n",
      "       [ 2.12603238e-08, -1.11332276e-08, -8.83513485e-10],\n",
      "       [-1.24218644e-07, -2.89642701e-08,  1.26122631e-07],\n",
      "       [-2.54867782e-08, -4.15968202e-08, -2.42752909e-08],\n",
      "       [-9.01846287e-08,  4.37781973e-08,  3.66413021e-08],\n",
      "       [ 1.01260034e-08,  1.94047924e-08, -5.66179676e-08],\n",
      "       [-6.21566578e-08,  8.03410354e-08,  2.02350040e-08],\n",
      "       [-5.96643860e-08, -2.53211397e-08,  5.42126004e-08],\n",
      "       [-2.39179162e-08,  1.45456706e-09, -1.94676770e-08],\n",
      "       [-4.14887920e-09,  2.09911237e-08,  5.38985205e-09],\n",
      "       [-1.76747310e-08, -5.43668414e-08, -4.21925284e-08],\n",
      "       [-1.72035279e-07, -2.82984835e-08,  1.41075148e-07],\n",
      "       [-1.29668046e-07, -2.23106607e-08,  9.33898163e-08],\n",
      "       [-1.62983811e-07,  8.52244579e-08,  8.91748330e-08],\n",
      "       [-1.63095856e-07,  5.86141110e-08,  4.68869676e-08],\n",
      "       [-1.47016193e-08, -3.85002780e-09, -1.67096537e-08],\n",
      "       [-7.11464000e-08,  1.28713113e-07,  2.02051955e-08],\n",
      "       [-4.02529517e-08, -5.11051819e-08, -7.77064464e-09],\n",
      "       [-5.37225377e-08, -5.65012310e-08, -4.09730779e-08],\n",
      "       [-4.60975519e-08,  1.05170824e-07, -3.05718619e-08],\n",
      "       [-1.31899792e-07,  6.91617020e-08,  8.52880100e-10],\n",
      "       [-5.16249212e-08,  1.39925512e-08, -3.82887565e-08],\n",
      "       [-1.05595065e-07,  1.05748622e-08, -6.19799630e-08],\n",
      "       [-9.82829727e-08,  6.51909453e-08, -6.66886903e-08],\n",
      "       [-7.67355656e-08, -1.33776174e-08, -1.14822859e-08],\n",
      "       [-1.66300305e-07,  3.99562473e-08,  9.07279464e-08],\n",
      "       [-1.60489476e-07,  5.01935302e-08,  2.02874006e-08],\n",
      "       [-1.14571228e-08, -4.95339063e-08,  3.37996838e-08],\n",
      "       [-2.98690968e-08, -2.19404592e-08,  2.28593365e-08],\n",
      "       [-2.65228170e-08, -1.55935877e-08, -7.16329098e-09],\n",
      "       [ 6.81289460e-08, -1.06496742e-07, -4.56329431e-08],\n",
      "       [-4.97731712e-09, -1.30206938e-07,  3.92792188e-08],\n",
      "       [-6.13837089e-08, -3.39234591e-08, -3.92200480e-08],\n",
      "       [ 3.03511257e-08, -6.73123608e-08, -2.81969498e-08],\n",
      "       [-5.87288035e-09,  4.65911728e-08, -2.73344789e-08],\n",
      "       [-4.97687794e-08, -1.87180073e-08,  1.32459138e-08],\n",
      "       [ 6.80676405e-08, -1.03970348e-07, -4.43188682e-08],\n",
      "       [-1.19115804e-07,  9.16899652e-08,  3.88625420e-09],\n",
      "       [-2.35641940e-08, -5.43020583e-10,  1.78267655e-09],\n",
      "       [-7.21670327e-08,  7.43002591e-08, -5.33351774e-09],\n",
      "       [ 7.14756332e-08, -1.11066409e-07, -5.54076159e-08],\n",
      "       [-1.56564419e-07,  6.03912563e-08,  1.11637421e-07],\n",
      "       [-7.50134438e-08, -2.01505035e-08, -1.06514654e-07],\n",
      "       [-3.44469821e-08,  2.72321124e-08,  2.38814396e-08],\n",
      "       [-1.12481442e-07,  1.16882752e-07,  6.27996632e-08],\n",
      "       [ 1.08758375e-08, -2.93701675e-08, -1.18791644e-08],\n",
      "       [ 3.22611053e-08, -6.03275003e-08, -4.37146423e-08],\n",
      "       [ 3.59747907e-08, -3.17418050e-08, -8.23200661e-08],\n",
      "       [-6.22586207e-08, -6.47501342e-08,  7.05205986e-08],\n",
      "       [-8.76768332e-08,  3.43317553e-08, -1.08950035e-07],\n",
      "       [ 2.35712582e-08, -3.36054306e-08, -8.15689517e-08],\n",
      "       [-1.22151683e-07, -6.30468148e-08,  7.60306423e-10],\n",
      "       [-8.03604492e-08, -9.38954091e-09, -1.48961626e-08],\n",
      "       [-5.51910478e-08, -7.89902460e-08, -9.47324917e-08],\n",
      "       [-3.01986953e-08, -3.33605010e-08, -4.84228377e-09],\n",
      "       [-8.70881995e-08,  8.12851067e-08,  3.14456772e-08],\n",
      "       [-6.98346999e-08, -4.48147946e-08, -7.20001297e-08],\n",
      "       [-3.79404524e-08,  7.06752409e-08, -2.95163111e-08],\n",
      "       [-1.00204199e-07,  5.03566036e-08,  6.31899982e-08],\n",
      "       [-1.62676292e-07, -1.90297784e-08,  1.00786533e-07],\n",
      "       [-7.55617349e-08,  3.42764560e-08, -4.77955958e-09],\n",
      "       [-1.28418850e-07,  1.48152669e-08, -1.90419769e-08],\n",
      "       [-2.59182774e-08, -2.70681377e-08, -1.23999863e-07],\n",
      "       [-3.17721034e-08, -1.27978512e-08,  1.54877652e-08],\n",
      "       [-2.48250187e-08,  4.04597380e-08, -3.21203477e-08],\n",
      "       [-1.37635773e-07,  9.66250574e-08, -6.23578941e-08],\n",
      "       [ 1.94262161e-08, -2.68513877e-08, -5.63932678e-08],\n",
      "       [-4.01451187e-08,  8.26498132e-08, -2.02118769e-08],\n",
      "       [-1.03163771e-07,  2.15205443e-08,  2.51154269e-08],\n",
      "       [-1.15542504e-07,  6.21291568e-08,  7.17700339e-08],\n",
      "       [-2.32724011e-08,  5.03257259e-08, -1.69207845e-08],\n",
      "       [-8.60014726e-08,  5.31983676e-08, -1.23434782e-07],\n",
      "       [-3.09411304e-07, -8.05989643e-08,  6.60743644e-08],\n",
      "       [ 3.33303835e-08, -2.34318246e-08, -1.17676887e-08]]))\n",
      "shift_scores: (100, 3) [[-1.48533882e-07 -9.47239692e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -3.03076895e-08 -8.68890804e-08]\n",
      " [ 0.00000000e+00 -5.03278397e-08 -1.39331171e-07]\n",
      " [ 0.00000000e+00 -4.14257482e-08 -4.41339673e-08]\n",
      " [-1.88913832e-07  0.00000000e+00 -1.13015235e-07]\n",
      " [-2.38407349e-07 -9.17205756e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -4.65985702e-08 -3.65286232e-09]\n",
      " [-4.06104083e-08  0.00000000e+00 -8.43614353e-08]\n",
      " [-2.70494357e-07 -1.17308293e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -7.55565816e-08 -1.28068690e-08]\n",
      " [-1.05683107e-07  0.00000000e+00 -1.24214360e-07]\n",
      " [-7.79388855e-08  0.00000000e+00 -8.14175310e-08]\n",
      " [-7.64995377e-08  0.00000000e+00 -2.30531171e-08]\n",
      " [-1.51019250e-07  0.00000000e+00 -7.60651175e-08]\n",
      " [-7.39961081e-08 -1.01897890e-07  0.00000000e+00]\n",
      " [-5.75448255e-08  0.00000000e+00 -5.20142095e-08]\n",
      " [-1.90685062e-07 -3.25091089e-08  0.00000000e+00]\n",
      " [-5.32118547e-08 -1.36241009e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -3.29543776e-08 -1.15935204e-07]\n",
      " [ 0.00000000e+00 -1.04492642e-07 -7.05718500e-08]\n",
      " [-2.03313110e-07  0.00000000e+00 -1.10667984e-07]\n",
      " [-9.12445951e-08  0.00000000e+00 -1.58763716e-07]\n",
      " [-1.90388675e-08  0.00000000e+00 -8.39771808e-09]\n",
      " [-2.63826400e-07 -1.59625060e-07  0.00000000e+00]\n",
      " [-1.72354959e-07 -1.69777564e-07  0.00000000e+00]\n",
      " [-4.69736460e-08 -1.01565202e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.38496906e-08 -2.35165479e-08]\n",
      " [ 0.00000000e+00 -3.23935514e-08 -2.21438373e-08]\n",
      " [-2.50341275e-07 -1.55086901e-07  0.00000000e+00]\n",
      " [-1.21148724e-09 -1.73215293e-08  0.00000000e+00]\n",
      " [-1.33962826e-07  0.00000000e+00 -7.13689516e-09]\n",
      " [-9.27878907e-09  0.00000000e+00 -7.60227600e-08]\n",
      " [-1.42497693e-07  0.00000000e+00 -6.01060315e-08]\n",
      " [-1.13876986e-07 -7.95337401e-08  0.00000000e+00]\n",
      " [-2.53724832e-08  0.00000000e+00 -2.09222440e-08]\n",
      " [-2.51400029e-08  0.00000000e+00 -1.56012717e-08]\n",
      " [ 0.00000000e+00 -3.66921104e-08 -2.45177973e-08]\n",
      " [-3.13110427e-07 -1.69373631e-07  0.00000000e+00]\n",
      " [-2.23057862e-07 -1.15700477e-07  0.00000000e+00]\n",
      " [-2.52158644e-07 -3.95037505e-09  0.00000000e+00]\n",
      " [-2.21709967e-07  0.00000000e+00 -1.17271433e-08]\n",
      " [-1.08515915e-08  0.00000000e+00 -1.28596259e-08]\n",
      " [-1.99859513e-07  0.00000000e+00 -1.08507918e-07]\n",
      " [-3.24823071e-08 -4.33345373e-08  0.00000000e+00]\n",
      " [-1.27494599e-08 -1.55281532e-08  0.00000000e+00]\n",
      " [-1.51268376e-07  0.00000000e+00 -1.35742686e-07]\n",
      " [-2.01061494e-07  0.00000000e+00 -6.83088219e-08]\n",
      " [-6.56174724e-08  0.00000000e+00 -5.22813077e-08]\n",
      " [-1.16169927e-07  0.00000000e+00 -7.25548252e-08]\n",
      " [-1.63473918e-07  0.00000000e+00 -1.31879636e-07]\n",
      " [-6.52532797e-08 -1.89533155e-09  0.00000000e+00]\n",
      " [-2.57028252e-07 -5.07716991e-08  0.00000000e+00]\n",
      " [-2.10683006e-07  0.00000000e+00 -2.99061296e-08]\n",
      " [-4.52568065e-08 -8.33335900e-08  0.00000000e+00]\n",
      " [-5.27284333e-08 -4.47997958e-08  0.00000000e+00]\n",
      " [-1.93595260e-08 -8.43029668e-09  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.74625688e-07 -1.13761889e-07]\n",
      " [-4.42565359e-08 -1.69486157e-07  0.00000000e+00]\n",
      " [-2.74602498e-08  0.00000000e+00 -5.29658890e-09]\n",
      " [ 0.00000000e+00 -9.76634865e-08 -5.85480756e-08]\n",
      " [-5.24640531e-08  0.00000000e+00 -7.39256517e-08]\n",
      " [-6.30146931e-08 -3.19639210e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.72037989e-07 -1.12386509e-07]\n",
      " [-2.10805769e-07  0.00000000e+00 -8.78037110e-08]\n",
      " [-2.53468705e-08 -2.32569713e-09  0.00000000e+00]\n",
      " [-1.46467292e-07  0.00000000e+00 -7.96337768e-08]\n",
      " [ 0.00000000e+00 -1.82542042e-07 -1.26883249e-07]\n",
      " [-2.68201840e-07 -5.12461652e-08  0.00000000e+00]\n",
      " [-5.48629403e-08  0.00000000e+00 -8.63641509e-08]\n",
      " [-6.16790945e-08  0.00000000e+00 -3.35067286e-09]\n",
      " [-2.29364195e-07  0.00000000e+00 -5.40830892e-08]\n",
      " [ 0.00000000e+00 -4.02460050e-08 -2.27550019e-08]\n",
      " [ 0.00000000e+00 -9.25886056e-08 -7.59757476e-08]\n",
      " [ 0.00000000e+00 -6.77165956e-08 -1.18294857e-07]\n",
      " [-1.32779219e-07 -1.35270733e-07  0.00000000e+00]\n",
      " [-1.22008588e-07  0.00000000e+00 -1.43281790e-07]\n",
      " [ 0.00000000e+00 -5.71766888e-08 -1.05140210e-07]\n",
      " [-1.22911989e-07 -6.38071213e-08  0.00000000e+00]\n",
      " [-7.09709083e-08  0.00000000e+00 -5.50662173e-09]\n",
      " [ 0.00000000e+00 -2.37991982e-08 -3.95414438e-08]\n",
      " [-2.53564116e-08 -2.85182173e-08  0.00000000e+00]\n",
      " [-1.68373306e-07  0.00000000e+00 -4.98394295e-08]\n",
      " [-2.50199053e-08  0.00000000e+00 -2.71853351e-08]\n",
      " [-1.08615693e-07  0.00000000e+00 -1.00191552e-07]\n",
      " [-1.63394197e-07 -1.28333946e-08  0.00000000e+00]\n",
      " [-2.63462826e-07 -1.19816312e-07  0.00000000e+00]\n",
      " [-1.09838191e-07  0.00000000e+00 -3.90560156e-08]\n",
      " [-1.43234117e-07  0.00000000e+00 -3.38572438e-08]\n",
      " [ 0.00000000e+00 -1.14986028e-09 -9.80815859e-08]\n",
      " [-4.72598686e-08 -2.82856164e-08  0.00000000e+00]\n",
      " [-6.52847567e-08  0.00000000e+00 -7.25800857e-08]\n",
      " [-2.34260831e-07  0.00000000e+00 -1.58982952e-07]\n",
      " [ 0.00000000e+00 -4.62776038e-08 -7.58194839e-08]\n",
      " [-1.22794932e-07  0.00000000e+00 -1.02861690e-07]\n",
      " [-1.28279198e-07 -3.59488263e-09  0.00000000e+00]\n",
      " [-1.87312538e-07 -9.64087719e-09  0.00000000e+00]\n",
      " [-7.35981270e-08  0.00000000e+00 -6.72465105e-08]\n",
      " [-1.39199840e-07  0.00000000e+00 -1.76633150e-07]\n",
      " [-3.75485668e-07 -1.46673329e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.67622081e-08 -4.50980721e-08]]\n",
      "('loss', 1.0986122897211157)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0986122897211157,\n",
       " {'W1': array([[ 2.90067446e-06, -4.89530885e-06, -1.09407808e-05,\n",
       "          -1.39567052e-06, -1.22325477e-06,  3.83709763e-07,\n",
       "           4.13133634e-06, -5.53564707e-07],\n",
       "         [ 1.41595914e-05,  6.08276380e-06, -8.57871102e-06,\n",
       "          -2.14142141e-06,  3.19081702e-06, -3.31898558e-06,\n",
       "          -1.52265487e-05,  3.12409743e-06],\n",
       "         [-3.36455402e-06, -6.64245881e-07,  1.20407476e-05,\n",
       "           1.80252903e-05, -2.32458520e-06, -2.44091520e-05,\n",
       "           7.45007121e-06,  6.88780451e-07],\n",
       "         [ 4.74415031e-06,  1.03502526e-06, -6.46190116e-06,\n",
       "          -1.51414087e-05,  1.98192662e-06,  1.89478726e-05,\n",
       "          -3.82441022e-06, -6.86271087e-06],\n",
       "         [ 6.25646146e-06,  1.02076156e-06,  1.53449837e-05,\n",
       "          -2.04439093e-06, -2.84132370e-07, -1.76004919e-06,\n",
       "          -3.22018685e-06,  1.27352345e-06],\n",
       "         [-7.85988852e-06, -4.39548085e-06, -1.15330167e-05,\n",
       "           1.07324757e-05,  5.23503739e-07,  1.12655385e-06,\n",
       "           8.35938869e-06, -1.03326277e-06],\n",
       "         [ 1.63171204e-05,  2.71394394e-06,  8.23479042e-06,\n",
       "           1.75242742e-07, -7.38284740e-07, -5.36106863e-06,\n",
       "          -8.43552295e-06,  1.07020903e-05],\n",
       "         [-3.54811817e-06,  3.12571329e-07, -2.00461049e-05,\n",
       "          -1.89453196e-05,  3.51690170e-06,  1.50838571e-05,\n",
       "          -2.60254847e-06,  1.00713142e-05],\n",
       "         [-5.72858877e-06,  2.24156092e-06, -2.11934479e-06,\n",
       "          -8.11655834e-06,  1.60611053e-06, -1.38877382e-06,\n",
       "           1.67003419e-06,  6.93329798e-07],\n",
       "         [ 1.72057186e-05, -1.27100501e-06, -5.65792914e-06,\n",
       "           4.57083908e-06, -1.57783982e-07, -2.48278642e-06,\n",
       "          -8.79476013e-06,  1.35979299e-05]]),\n",
       "  'W2': array([[-1.94070081e-05,  8.53381865e-06,  1.08731894e-05],\n",
       "         [-3.09855159e-06,  8.24826431e-06, -5.14971272e-06],\n",
       "         [ 2.87938259e-06,  4.43403445e-07, -3.32278603e-06],\n",
       "         [ 1.82688285e-05, -2.43366757e-05,  6.06784717e-06],\n",
       "         [ 2.25855837e-06, -7.47163025e-06,  5.21307187e-06],\n",
       "         [ 2.49913863e-05, -1.86966090e-05, -6.29477724e-06],\n",
       "         [ 1.80094952e-05, -1.85935757e-05,  5.84080425e-07],\n",
       "         [ 1.36010886e-05, -2.93994294e-05,  1.57983407e-05]]),\n",
       "  'b1': array([ 8.48033305e-06, -4.72169161e-07,  5.77479254e-06,  1.32863730e-05,\n",
       "          7.33611272e-07, -1.44973509e-05,  8.61555051e-06, -1.42141423e-05]),\n",
       "  'b2': array([ 0.04333332, -0.06666666,  0.02333334])})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.loss(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
