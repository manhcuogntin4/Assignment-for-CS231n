{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network. The net has an input dimension of\n",
    "  N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "  We train the network with a softmax loss function and L2 regularization on the\n",
    "  weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "  connected layer.\n",
    "  In other words, the network has the following architecture:\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights are initialized to small random values and\n",
    "    biases are initialized to zero. Weights and biases are stored in the\n",
    "    variable self.params, which is a dictionary with the following keys:\n",
    "    W1: First layer weights; has shape (D, H)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (H, C)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "    Inputs:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a two layer fully connected neural\n",
    "    network.\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "      is not passed then we only return scores, and if it is passed then we\n",
    "      instead return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "    Returns:\n",
    "    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "    If y is not None, instead return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "    #############################################################################\n",
    "    # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "    # Store the result in the scores variable, which should be an array of      #\n",
    "    # shape (N, C).                                                             #\n",
    "    #############################################################################\n",
    "    h_output = np.maximum(0, X.dot(W1) + b1) #(N,D) * (D,H) = (N,H)\n",
    "    print(\"h_output\",h_output.shape)\n",
    "    scores = h_output.dot(W2) + b2\n",
    "    print(\"scores\", scores.shape)\n",
    "    # pass\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "    \n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = None\n",
    "    #############################################################################\n",
    "    # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "    # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "    # classifier loss. So that your results match ours, multiply the            #\n",
    "    # regularization loss by 0.5                                                #\n",
    "    #############################################################################\n",
    "    shift_scores = scores - np.max(scores, axis = 1).reshape(-1,1)\n",
    "    print \"shift_scores:\",shift_scores.shape, shift_scores\n",
    "    softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores), axis = 1).reshape(-1,1)\n",
    "    print \"softmax_output\",softmax_output\n",
    "    loss = -np.sum(np.log(softmax_output[range(N), list(y)]))\n",
    "    loss /= N\n",
    "    loss +=  0.5* reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "    print(\"loss\",loss)\n",
    "    # pass\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    grads = {}\n",
    "    #############################################################################\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "    #############################################################################\n",
    "    # pass\n",
    "    dscores = softmax_output.copy()\n",
    "    dscores[range(N), list(y)] -= 1\n",
    "    print \"dscores\",dscores\n",
    "    dscores /= N\n",
    "    grads['W2'] = h_output.T.dot(dscores) + reg * W2\n",
    "    grads['b2'] = np.sum(dscores, axis = 0)\n",
    "    \n",
    "    dh = dscores.dot(W2.T)\n",
    "    dh_ReLu = (h_output > 0) * dh\n",
    "    grads['W1'] = X.T.dot(dh_ReLu) + reg * W1\n",
    "    grads['b1'] = np.sum(dh_ReLu, axis = 0)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in xrange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "      # them in X_batch and y_batch respectively.                             #\n",
    "      #########################################################################\n",
    "      idx = np.random.choice(num_train, batch_size, replace=True)\n",
    "      X_batch = X[idx]\n",
    "      y_batch = y[idx]\n",
    "      # pass\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "      # parameters of the network (stored in the dictionary self.params)      #\n",
    "      # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "      # stored in the grads dictionary defined above.                         #\n",
    "      #########################################################################\n",
    "      self.params['W2'] += - learning_rate * grads['W2']\n",
    "      self.params['b2'] += - learning_rate * grads['b2']\n",
    "      self.params['W1'] += - learning_rate * grads['W1']\n",
    "      self.params['b1'] += - learning_rate * grads['b1']\n",
    "      # pass\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print 'iteration %d / %d: loss %f' % (it, num_iters, loss)\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "    y_pred = None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement this function; it should be VERY simple!                #\n",
    "    ###########################################################################\n",
    "    h = np.maximum(0, X.dot(self.params['W1']) + self.params['b1'])\n",
    "    scores = h.dot(self.params['W2']) + self.params['b2']\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    # pass\n",
    "    ###########################################################################\n",
    "    #                              END OF YOUR CODE                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl=TwoLayerNet(10,8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.params['b2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.random.randn(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=np.random.randint(3, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 1, 0, 2,\n",
       "       2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 2, 0, 1,\n",
       "       1, 1, 2, 2, 0, 1, 2, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1,\n",
       "       2, 2, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 2,\n",
       "       0, 2, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h_output', (100, 8))\n",
      "('scores', (100, 3))\n",
      "shift_scores: (100, 3) [[-2.20605117e-08 -8.72239366e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -2.15748614e-07 -5.62545151e-08]\n",
      " [-7.71148023e-08 -9.87536832e-08  0.00000000e+00]\n",
      " [-1.83632419e-08 -7.50145719e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -9.57671488e-08 -1.34629267e-08]\n",
      " [-4.28170101e-08 -9.78811672e-09  0.00000000e+00]\n",
      " [-5.61381744e-10 -1.63726485e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.51257263e-07 -2.59228395e-08]\n",
      " [-3.28381492e-08  0.00000000e+00 -2.25631777e-08]\n",
      " [ 0.00000000e+00 -4.04166900e-08 -2.64623962e-09]\n",
      " [-3.79671774e-08 -1.37372767e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.95219091e-07 -3.17056308e-08]\n",
      " [-9.37269680e-09  0.00000000e+00 -1.06059367e-07]\n",
      " [-5.83571373e-09 -1.12524369e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.49854384e-08 -1.35215367e-08]\n",
      " [-5.94114721e-09 -6.83962786e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -6.86341834e-08 -4.82569158e-09]\n",
      " [-2.48044284e-08 -4.24280698e-08  0.00000000e+00]\n",
      " [-1.06932502e-08  0.00000000e+00 -1.70988492e-08]\n",
      " [-1.43768750e-08 -7.02354262e-08  0.00000000e+00]\n",
      " [-2.39921586e-08 -7.29917654e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -3.72639811e-08 -9.06992865e-09]\n",
      " [-3.70433728e-08 -1.33429850e-07  0.00000000e+00]\n",
      " [-1.54177882e-08 -5.25500948e-08  0.00000000e+00]\n",
      " [-4.40101399e-08 -6.06015363e-08  0.00000000e+00]\n",
      " [-3.60136880e-08  0.00000000e+00 -1.32673640e-08]\n",
      " [-2.26219050e-08  0.00000000e+00 -8.15483839e-09]\n",
      " [ 0.00000000e+00 -9.77749104e-08 -1.72141282e-08]\n",
      " [ 0.00000000e+00 -1.26946382e-07 -1.44325204e-08]\n",
      " [ 0.00000000e+00 -4.97355612e-08 -1.40129236e-08]\n",
      " [ 0.00000000e+00 -5.95287804e-09 -1.67951265e-08]\n",
      " [ 0.00000000e+00 -1.21866934e-07 -2.99427183e-08]\n",
      " [-1.94649223e-08 -2.96618635e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -4.64664276e-08 -2.30855429e-08]\n",
      " [ 0.00000000e+00 -1.41040777e-07 -4.01425409e-09]\n",
      " [ 0.00000000e+00 -6.23369873e-08 -7.24029594e-09]\n",
      " [-2.94233908e-08 -1.68540985e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -5.04264065e-08 -3.51299551e-08]\n",
      " [ 0.00000000e+00 -3.08683017e-08 -1.39548154e-08]\n",
      " [ 0.00000000e+00 -4.83410160e-08 -4.41789218e-09]\n",
      " [-1.48104789e-08 -1.53521000e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.73174966e-07 -2.46465463e-08]\n",
      " [-1.44315470e-08 -1.59130665e-08  0.00000000e+00]\n",
      " [-3.78751475e-08  0.00000000e+00 -2.05917470e-08]\n",
      " [ 0.00000000e+00 -1.30415494e-07 -1.76738406e-08]\n",
      " [-1.72558817e-08 -2.91074707e-08  0.00000000e+00]\n",
      " [-1.01500087e-08 -3.75930815e-08  0.00000000e+00]\n",
      " [-2.71546675e-08  0.00000000e+00 -4.00850762e-08]\n",
      " [-3.21052978e-08 -9.53188657e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -7.24704957e-09 -1.91368394e-09]\n",
      " [-1.42003738e-08 -2.44221634e-08  0.00000000e+00]\n",
      " [-6.00893258e-09 -7.61041488e-08  0.00000000e+00]\n",
      " [-2.37951010e-10 -2.58484557e-10  0.00000000e+00]\n",
      " [ 0.00000000e+00 -9.25266012e-08 -1.71769103e-08]\n",
      " [-5.31258495e-08 -1.68709238e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.80279824e-07 -2.76364746e-08]\n",
      " [ 0.00000000e+00 -8.22709076e-08 -6.22815966e-08]\n",
      " [-5.13158594e-08 -6.63839272e-08  0.00000000e+00]\n",
      " [-1.61674272e-08 -7.13189699e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -2.10229130e-08 -4.05758744e-08]\n",
      " [-4.26258814e-08 -4.61046963e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -6.62907602e-08 -1.43168814e-08]\n",
      " [ 0.00000000e+00 -1.20134169e-07 -2.34547441e-08]\n",
      " [-3.50301239e-08 -8.84165191e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.62795269e-07 -4.64421292e-08]\n",
      " [ 0.00000000e+00 -1.25885601e-08 -3.32375064e-08]\n",
      " [-4.42588508e-10 -7.41185515e-08  0.00000000e+00]\n",
      " [-6.56432516e-08  0.00000000e+00 -2.52745562e-08]\n",
      " [-1.07035229e-08 -8.96789391e-08  0.00000000e+00]\n",
      " [-6.17203712e-09 -2.06754428e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -3.51726696e-09 -1.93855584e-08]\n",
      " [-3.79147022e-08  0.00000000e+00 -2.60868627e-08]\n",
      " [-3.57891854e-09 -8.49263927e-09  0.00000000e+00]\n",
      " [-8.17712210e-09  0.00000000e+00 -2.37138007e-08]\n",
      " [-1.97063199e-08 -1.10409502e-07  0.00000000e+00]\n",
      " [-3.23539823e-08 -6.44835457e-08  0.00000000e+00]\n",
      " [-7.93347757e-09 -1.00795491e-07  0.00000000e+00]\n",
      " [-1.26141696e-08 -1.42586015e-07  0.00000000e+00]\n",
      " [-5.32135607e-09 -1.09160944e-07  0.00000000e+00]\n",
      " [-8.93182648e-08 -4.08891980e-08  0.00000000e+00]\n",
      " [-1.33266875e-08 -1.71814642e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -7.40282639e-08 -7.65858453e-09]\n",
      " [-2.60652647e-08 -9.48931978e-08  0.00000000e+00]\n",
      " [-2.33153610e-08 -1.75890477e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.70044406e-07 -1.88799191e-08]\n",
      " [-3.58143652e-09 -9.56913680e-08  0.00000000e+00]\n",
      " [-5.88396075e-08 -1.44753265e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -2.15912866e-08 -2.83664437e-08]\n",
      " [-3.99061586e-09  0.00000000e+00 -2.77130261e-08]\n",
      " [-1.16412491e-07 -1.12585791e-07  0.00000000e+00]\n",
      " [-7.40851321e-09 -6.49986827e-08  0.00000000e+00]\n",
      " [-4.25371635e-08  0.00000000e+00 -4.11300858e-08]\n",
      " [-5.61759406e-08 -1.51338777e-07  0.00000000e+00]\n",
      " [-2.84163193e-08 -1.89710115e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -3.59991666e-08 -1.09585421e-08]\n",
      " [-1.61452330e-08 -5.54344238e-08  0.00000000e+00]\n",
      " [-2.16887792e-08 -6.31051720e-08  0.00000000e+00]\n",
      " [-2.94535452e-09  0.00000000e+00 -4.78425075e-08]\n",
      " [-2.62860586e-08 -3.68336630e-08  0.00000000e+00]\n",
      " [ 0.00000000e+00 -2.20463541e-09 -2.31237343e-08]]\n",
      "softmax_output [[0.33333334 0.33333332 0.33333335]\n",
      " [0.33333336 0.33333329 0.33333334]\n",
      " [0.33333333 0.33333332 0.33333335]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333332 0.33333334 0.33333334]\n",
      " [0.33333335 0.3333333  0.33333335]\n",
      " [0.33333335 0.3333333  0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333331 0.33333335]\n",
      " [0.33333336 0.33333329 0.33333335]\n",
      " [0.33333334 0.33333335 0.33333331]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333331 0.33333335]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333335 0.3333333  0.33333335]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333336 0.3333333  0.33333335]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333335]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333334 0.3333333  0.33333336]\n",
      " [0.33333336 0.3333333  0.33333335]\n",
      " [0.33333335 0.33333332 0.33333333]\n",
      " [0.33333333 0.33333332 0.33333335]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333335 0.33333331 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333335]\n",
      " [0.33333336 0.3333333  0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333332 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333331 0.33333334]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333334 0.33333331 0.33333335]\n",
      " [0.33333333 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333331 0.33333335]\n",
      " [0.33333335 0.3333333  0.33333335]\n",
      " [0.33333334 0.33333331 0.33333335]\n",
      " [0.33333332 0.33333333 0.33333335]\n",
      " [0.33333335 0.3333333  0.33333335]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333335]\n",
      " [0.33333335 0.3333333  0.33333336]\n",
      " [0.33333335 0.3333333  0.33333335]\n",
      " [0.33333334 0.33333331 0.33333334]\n",
      " [0.33333332 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333334 0.33333334 0.33333333]\n",
      " [0.33333332 0.33333332 0.33333336]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333333 0.33333334 0.33333333]\n",
      " [0.33333334 0.33333331 0.33333336]\n",
      " [0.33333335 0.33333329 0.33333336]\n",
      " [0.33333334 0.33333333 0.33333333]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333332 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333332]\n",
      " [0.33333333 0.33333333 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333333]]\n",
      "('loss', 1.0986122981630408)\n",
      "dscores [[-0.66666666  0.33333332  0.33333335]\n",
      " [-0.66666664  0.33333329  0.33333334]\n",
      " [ 0.33333333 -0.66666668  0.33333335]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [ 0.33333335  0.33333331 -0.66666666]\n",
      " [ 0.33333332  0.33333334 -0.66666666]\n",
      " [-0.66666665  0.3333333   0.33333335]\n",
      " [ 0.33333335 -0.6666667   0.33333334]\n",
      " [-0.66666667  0.33333334  0.33333333]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [ 0.33333334 -0.66666669  0.33333335]\n",
      " [-0.66666664  0.33333329  0.33333335]\n",
      " [ 0.33333334 -0.66666665  0.33333331]\n",
      " [-0.66666667  0.33333333  0.33333334]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [-0.66666666  0.33333332  0.33333334]\n",
      " [-0.66666667  0.33333333  0.33333334]\n",
      " [ 0.33333333 -0.66666666  0.33333333]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [-0.66666666  0.33333332  0.33333334]\n",
      " [ 0.33333334  0.33333333 -0.66666666]\n",
      " [ 0.33333334  0.33333331 -0.66666665]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [ 0.33333333 -0.66666668  0.33333334]\n",
      " [ 0.33333333  0.33333334 -0.66666667]\n",
      " [ 0.33333333 -0.66666666  0.33333333]\n",
      " [ 0.33333335 -0.66666669  0.33333334]\n",
      " [ 0.33333335 -0.66666669  0.33333334]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [-0.66666666  0.33333333  0.33333333]\n",
      " [ 0.33333335 -0.66666669  0.33333334]\n",
      " [ 0.33333333 -0.66666667  0.33333334]\n",
      " [ 0.33333334 -0.66666667  0.33333333]\n",
      " [ 0.33333335 -0.6666667   0.33333335]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [-0.66666667  0.33333333  0.33333334]\n",
      " [ 0.33333334 -0.66666667  0.33333333]\n",
      " [ 0.33333334  0.33333333 -0.66666667]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333333 -0.66666667  0.33333334]\n",
      " [ 0.33333336  0.3333333  -0.66666665]\n",
      " [-0.66666667  0.33333333  0.33333334]\n",
      " [ 0.33333333 -0.66666666  0.33333333]\n",
      " [ 0.33333335 -0.66666669  0.33333334]\n",
      " [ 0.33333333 -0.66666667  0.33333334]\n",
      " [ 0.33333334  0.33333333 -0.66666666]\n",
      " [ 0.33333333  0.33333334 -0.66666667]\n",
      " [-0.66666666  0.33333332  0.33333335]\n",
      " [ 0.33333333 -0.66666667  0.33333333]\n",
      " [ 0.33333333  0.33333333 -0.66666666]\n",
      " [-0.66666666  0.33333332  0.33333334]\n",
      " [ 0.33333333  0.33333333 -0.66666667]\n",
      " [ 0.33333335 -0.66666669  0.33333334]\n",
      " [ 0.33333334 -0.6666667   0.33333336]\n",
      " [ 0.33333336 -0.6666667   0.33333335]\n",
      " [ 0.33333335  0.33333332 -0.66666667]\n",
      " [ 0.33333333  0.33333332 -0.66666665]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333334  0.33333333 -0.66666667]\n",
      " [ 0.33333333 -0.66666667  0.33333334]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [ 0.33333335  0.33333331 -0.66666666]\n",
      " [ 0.33333334  0.33333332 -0.66666665]\n",
      " [ 0.33333336 -0.6666667   0.33333334]\n",
      " [ 0.33333334 -0.66666667  0.33333333]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333332  0.33333334 -0.66666666]\n",
      " [ 0.33333334 -0.66666669  0.33333334]\n",
      " [-0.66666667  0.33333333  0.33333334]\n",
      " [ 0.33333334 -0.66666667  0.33333333]\n",
      " [ 0.33333333 -0.66666666  0.33333333]\n",
      " [-0.66666667  0.33333333  0.33333333]\n",
      " [-0.66666667  0.33333334  0.33333333]\n",
      " [ 0.33333334 -0.66666669  0.33333335]\n",
      " [-0.66666667  0.33333332  0.33333334]\n",
      " [ 0.33333334  0.33333331 -0.66666665]\n",
      " [ 0.33333335 -0.6666667   0.33333335]\n",
      " [ 0.33333334 -0.66666669  0.33333335]\n",
      " [ 0.33333332 -0.66666667  0.33333335]\n",
      " [-0.66666665  0.3333333   0.33333335]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333334 -0.66666668  0.33333335]\n",
      " [ 0.33333335 -0.6666667   0.33333336]\n",
      " [ 0.33333335 -0.6666667   0.33333335]\n",
      " [ 0.33333334  0.33333331 -0.66666666]\n",
      " [-0.66666668  0.33333334  0.33333334]\n",
      " [ 0.33333334  0.33333333 -0.66666667]\n",
      " [-0.66666666  0.33333334  0.33333333]\n",
      " [ 0.33333332  0.33333332 -0.66666664]\n",
      " [-0.66666666  0.33333332  0.33333334]\n",
      " [ 0.33333333  0.33333334 -0.66666667]\n",
      " [ 0.33333334 -0.66666669  0.33333336]\n",
      " [ 0.33333335  0.33333329 -0.66666664]\n",
      " [ 0.33333334 -0.66666667  0.33333333]\n",
      " [ 0.33333334 -0.66666668  0.33333334]\n",
      " [ 0.33333334  0.33333332 -0.66666666]\n",
      " [ 0.33333334 -0.66666666  0.33333332]\n",
      " [ 0.33333333 -0.66666667  0.33333334]\n",
      " [ 0.33333334 -0.66666666  0.33333333]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0986122981630408,\n",
       " {'W1': array([[ 3.63416350e-06,  1.90500405e-06,  2.94076364e-07,\n",
       "          -1.17497880e-06, -4.36599708e-06, -8.00811456e-07,\n",
       "          -2.25240646e-06,  1.36351139e-06],\n",
       "         [ 5.46958052e-06, -1.38084857e-06, -1.03789689e-05,\n",
       "           4.06472026e-08,  2.35884680e-06,  1.21770002e-06,\n",
       "           2.66631207e-06, -1.58554571e-06],\n",
       "         [ 2.97805880e-06, -9.66781053e-07, -2.25949326e-06,\n",
       "          -1.31107823e-06, -5.82026041e-06, -9.00008168e-07,\n",
       "           6.32396686e-07,  4.95317189e-06],\n",
       "         [-1.07735000e-05, -2.35928405e-07,  2.66871669e-06,\n",
       "          -3.71902553e-08, -1.24311897e-05, -1.49225068e-06,\n",
       "          -5.41433351e-06,  1.13381026e-06],\n",
       "         [ 6.39217536e-06,  1.64338997e-06, -2.52236871e-06,\n",
       "           1.36772474e-06,  3.45995553e-06, -8.80504643e-07,\n",
       "           4.31118425e-06,  1.46435676e-06],\n",
       "         [-1.59512758e-06,  2.27736822e-06,  6.84494357e-07,\n",
       "           1.83850533e-06,  1.20757049e-05, -1.53089367e-06,\n",
       "           5.22246162e-06, -1.45175972e-06],\n",
       "         [ 4.66251689e-06, -2.73588943e-07, -1.06546092e-06,\n",
       "           1.78990545e-06,  7.96106744e-06,  1.32557232e-06,\n",
       "           3.98643824e-06, -1.85527303e-06],\n",
       "         [-7.71280461e-06,  9.27903482e-07,  8.93407849e-07,\n",
       "           3.72491086e-06,  4.64601148e-06, -3.19976266e-06,\n",
       "          -1.10219429e-05,  1.95494851e-06],\n",
       "         [-3.00796787e-06,  7.49957170e-07, -3.25673056e-06,\n",
       "           1.67669787e-07, -1.47048993e-05,  6.71840556e-07,\n",
       "          -2.37848059e-06, -3.52223074e-07],\n",
       "         [-4.72316464e-06,  2.43450284e-06, -6.99594966e-06,\n",
       "           1.24503182e-06, -3.78961806e-06, -1.03212942e-06,\n",
       "          -6.78064301e-06,  4.06386545e-06]]),\n",
       "  'W2': array([[ 1.51732201e-05, -2.61968001e-05,  1.10235799e-05],\n",
       "         [ 2.43243806e-05, -2.90977918e-05,  4.77341117e-06],\n",
       "         [ 6.31268394e-07,  5.93354217e-07, -1.22462261e-06],\n",
       "         [ 7.92769394e-06, -1.50023025e-05,  7.07460856e-06],\n",
       "         [ 1.30179831e-05, -1.68300229e-05,  3.81203979e-06],\n",
       "         [ 1.36507071e-05, -8.23298914e-06, -5.41771795e-06],\n",
       "         [ 1.13101464e-05, -1.72186246e-05,  5.90847814e-06],\n",
       "         [ 9.60846575e-06, -1.54123765e-05,  5.80391071e-06]]),\n",
       "  'b1': array([ 7.03844531e-06,  1.62498988e-06, -4.69207351e-06,  4.06038194e-06,\n",
       "          1.52802155e-05, -1.32596793e-06,  6.80512589e-06, -2.69567824e-06]),\n",
       "  'b2': array([ 0.11333334, -0.13666668,  0.02333334])})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.loss(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10), list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
